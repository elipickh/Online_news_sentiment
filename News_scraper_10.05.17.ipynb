{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests, time, os, copy, re, random, uuid, subprocess\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from time import strptime\n",
    "from datetime import datetime\n",
    "import urllib.request\n",
    "from newspaper import Config, Article, Source\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm, trange\n",
    "from nltk import tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wait_for_internet_connection():\n",
    "    while True:\n",
    "        try:\n",
    "            response = urllib.request.urlopen('https://facebook.com',timeout=1)\n",
    "            return\n",
    "        except urllib.request.URLError:\n",
    "            pass\n",
    "        \n",
    "def pause():\n",
    "    programPause = input(\"Press the <ENTER> key to continue...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forbes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Scrape URL's from Google Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chromedriver = '/Users/elipickh/Downloads/chromedriver'\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "\n",
    "#chrome_options = Options()\n",
    "#adblock:\n",
    "#chrome_options.add_extension('/Users/elipickh/Downloads/extension_1_13_3.crx')\n",
    "#chrome_options.add_argument('headless')\n",
    "#driver = webdriver.Chrome(chromedriver, chrome_options=chrome_options)\n",
    "\n",
    "driver = webdriver.Chrome(chromedriver)                                    \n",
    "\n",
    "appended_data = []\n",
    "appended_data_0 = []\n",
    "appended_data_1 = []\n",
    "\n",
    "first = True\n",
    "bot_warn = 'Our systems have detected unusual traffic from your computer network'\n",
    "\n",
    "for x in range(0, 1):\n",
    "#for x in range(0, 2):    \n",
    "    for y in range(4, 8):\n",
    "        for m in range (1, 13):\n",
    "            for d in range(1, 32):\n",
    "                wait_for_internet_connection()\n",
    "                driver.get('https://www.google.com/search?q=' + str(uuid.uuid4()))\n",
    "                #time.sleep(random.uniform(1, 1.5))\n",
    "                wait_for_internet_connection()\n",
    "                driver.get('https://www.google.com/search?q=site:www.forbes.com/sites/*/201' + str(y) +'/' + str('{0:02}'.format(m)) + '/' + str('{0:02}'.format(d)) + '/' +'&filter=' + str(x))\n",
    "                src = driver.page_source\n",
    "\n",
    "                if first:\n",
    "                    pause()\n",
    "                    first = False\n",
    "                elif bool(re.search(bot_warn, src)):\n",
    "                    os.system(\"printf '\\a'\"); time.sleep(1); os.system(\"printf '\\a'\"); time.sleep(1); os.system(\"printf '\\a'\")\n",
    "                    pause()\n",
    "                #else:\n",
    "                    #time.sleep(random.uniform(61, 63))\n",
    "                    #time.sleep(random.uniform(1, 1.5))\n",
    "\n",
    "                while True:\n",
    "                    for i in range(1, 150):\n",
    "                        try:\n",
    "                            \n",
    "                            url = driver.find_element_by_css_selector('.g:nth-child(' + str(i) +\") a\").get_attribute('href')\n",
    "                            appended_data.append(url)\n",
    "                            \n",
    "                            if x == 0:\n",
    "                                appended_data_0.append(url)\n",
    "                            if x == 1:\n",
    "                                appended_data_1.append(url)\n",
    "\n",
    "                        except:\n",
    "                            #print(\"no more links\")\n",
    "                            break\n",
    "                    #time.sleep(random.uniform(0.5, 1))\n",
    "\n",
    "                    try: \n",
    "                        # this is navigate to next page \n",
    "                        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                        time.sleep(random.uniform(0.5, 1))\n",
    "                        button = driver.find_element_by_css_selector('#pnnext .ch+ span')\n",
    "                        wait_for_internet_connection()\n",
    "                        button.click() \n",
    "                        #time.sleep(random.uniform(2, 3))\n",
    "                    except: \n",
    "                        #print(\"no more pages\")\n",
    "                        break\n",
    "                if x == 0:\n",
    "                    print(str(len(set(appended_data_0))) + '  ' + str(m) + '/' + str(d) + '--' + str(y))\n",
    "                if x == 1:\n",
    "                    print(str(len(set(appended_data_1))) + '  ' + str(m) + '/' + str(d) + '--' + str(y))\n",
    "\n",
    "\n",
    "# only in filter=0\n",
    "print(len(list(set(appended_data_0).difference(appended_data_1))))\n",
    "# only in filter=1\n",
    "print(len(list(set(appended_data_1).difference(appended_data_0))))\n",
    "# in both\n",
    "print(len(list(set(appended_data_0).intersection(appended_data_1))))\n",
    "\n",
    "appended_data2 = set(appended_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(appended_data_0, columns=[\"colummn\"])\n",
    "df.to_csv('forbes_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forbes_data = pd.read_csv(\"forbes_list.csv\", header=0)\n",
    "forbes_urls = list(forbes_data.colummn)\n",
    "len(set(forbes_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forbes_urls = [\"/\".join(x.split(\"/\")[:9]) for x in forbes_urls]\n",
    "forbes_urls = [x+'/' for x in forbes_urls]\n",
    "forbes_urls = [x.replace('http:/','https:/') for x in forbes_urls]\n",
    "forbes_urls = list(set(forbes_urls))\n",
    "len(forbes_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Fetch article content from URL's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = Config()\n",
    "config.MAX_TEXT = 1000000\n",
    "\n",
    "def chunkify(lst,n):\n",
    "    return [ lst[i::n] for i in range(n) ]\n",
    "\n",
    "def process_article(url):\n",
    "    for x in range(0, 5):\n",
    "        try:\n",
    "            article = Article(url, config)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "\n",
    "            headline = article.title\n",
    "            publish_date = article.publish_date\n",
    "            authors = article.authors\n",
    "            fulltext = article.text\n",
    "            article.nlp()\n",
    "            keywords = article.keywords\n",
    "            summary = article.summary\n",
    "            forbes_all = {'url':url, \n",
    "                          'date':publish_date, \n",
    "                          'authors':authors, \n",
    "                          'headline':headline, \n",
    "                          'fulltext':fulltext, \n",
    "                          'summary':summary, \n",
    "                          'keywords':keywords}\n",
    "            return forbes_all\n",
    "        except:\n",
    "            wait_for_internet_connection()\n",
    "            time.sleep(0.1)\n",
    "            continue\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = Parallel(n_jobs=-1)(delayed(process_article)(url) for url in tqdm(forbes_urls))\n",
    "\n",
    "results = [x for x in results if x is not None]\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('forbes_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forbes_output = pd.read_csv(\"forbes_output.csv\", header=0)\n",
    "forbes_output.drop('summary', axis=1, inplace=True)\n",
    "forbes_output_list = forbes_output.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(s):\n",
    "    return str(s.encode('ascii','ignore'), 'utf-8').replace('\\n', ' ')\n",
    "\n",
    "for d in forbes_output_list:\n",
    "    for k, v in d.items():\n",
    "        if k=='fulltext' or k=='headline':\n",
    "            d[k]=clean_text(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for d in forbes_output_list:\n",
    "    for k, v in list(d.items()):\n",
    "        if k=='fulltext':\n",
    "            d['sentences']=tokenize.sent_tokenize(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "for d in forbes_output_list:\n",
    "    for k, v in list(d.items()):\n",
    "        if k=='headline':\n",
    "            #for v in v:\n",
    "            d['headline_score']=sid.polarity_scores(v)\n",
    "        if k=='sentences':\n",
    "            a=[]\n",
    "            for v in v:\n",
    "                a.append(sid.polarity_scores(v))  \n",
    "                d['fulltext_scores'] = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for maindict in forbes_output_list:\n",
    "    compound_list = []\n",
    "    pos_list =[]\n",
    "    neg_list =[]\n",
    "    neu_list =[]\n",
    "    compound_list_nonzero = []\n",
    "    pos_list_nonzero =[]\n",
    "    neg_list_nonzero =[]\n",
    "    neu_list_nonzero =[]    \n",
    "    for subdict in maindict['fulltext_scores']:\n",
    "        compound_list.append(subdict['compound'])\n",
    "        pos_list.append(subdict['pos'])\n",
    "        neg_list.append(subdict['neg'])\n",
    "        neu_list.append(subdict['neu'])\n",
    "        if subdict['compound']!=0:\n",
    "            compound_list_nonzero.append(subdict['compound'])\n",
    "        if subdict['pos']!=0:\n",
    "            pos_list_nonzero.append(subdict['pos'])\n",
    "        if subdict['neg']!=0:\n",
    "            neg_list_nonzero.append(subdict['neg'])\n",
    "        if subdict['neu']!=0:\n",
    "            neu_list_nonzero.append(subdict['neu'])  \n",
    "    \n",
    "    maindict['mean_compound'] = np.mean(compound_list)\n",
    "    maindict['median_compound'] = np.median(compound_list)\n",
    "    maindict['mean_pos'] = np.mean(pos_list)\n",
    "    maindict['median_pos'] = np.median(pos_list)\n",
    "    maindict['mean_neg'] = np.mean(neg_list)\n",
    "    maindict['median_neg'] = np.median(neg_list)\n",
    "    maindict['mean_neu'] = np.mean(neu_list)\n",
    "    maindict['median_neu'] = np.median(neu_list)\n",
    "    \n",
    "    maindict['mean_compound_nonzero'] = np.mean(compound_list_nonzero)\n",
    "    maindict['median_compound_nonzero'] = np.median(compound_list_nonzero)\n",
    "    maindict['mean_pos_nonzero'] = np.mean(pos_list_nonzero)\n",
    "    maindict['median_pos_nonzero'] = np.median(pos_list_nonzero)\n",
    "    maindict['mean_neg_nonzero'] = np.mean(neg_list_nonzero)\n",
    "    maindict['median_neg_nonzero'] = np.median(neg_list_nonzero)\n",
    "    maindict['mean_neu_nonzero'] = np.mean(neu_list_nonzero)\n",
    "    maindict['median_neu_nonzero'] = np.median(neu_list_nonzero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(forbes_output_list)\n",
    "df2.to_csv('forbes_output_list_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Get page views and categories for each article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Doing below in Ubuntu EC2\n",
    "# Block the following domains from, by entering: sudo nano /etc/hosts\n",
    "# and then pasting these lines in the bottom: (don't include facebook, as it's used to test for internet)\n",
    "127.0.0.1 bit.ly\n",
    "127.0.0.1 truste.com\n",
    "127.0.0.1 trefis.com\n",
    "127.0.0.1 twitter.com\n",
    "127.0.0.1 google.com\n",
    "127.0.0.1 google-analytics.com\n",
    "127.0.0.1 googletagmanager.com\n",
    "127.0.0.1 doubleclick.net\n",
    "127.0.0.1 youtube.com\n",
    "127.0.0.1 googlesyndication.com\n",
    "127.0.0.1 brightcove.com\n",
    "127.0.0.1 media.net\n",
    "127.0.0.1 www.bit.ly\n",
    "127.0.0.1 www.truste.com\n",
    "127.0.0.1 www.trefis.com\n",
    "127.0.0.1 www.twitter.com\n",
    "127.0.0.1 www.google.com\n",
    "127.0.0.1 www.google-analytics.com\n",
    "127.0.0.1 www.googletagmanager.com\n",
    "127.0.0.1 www.doubleclick.net\n",
    "127.0.0.1 www.youtube.com\n",
    "127.0.0.1 www.googlesyndication.com\n",
    "127.0.0.1 www.brightcove.com\n",
    "127.0.0.1 www.media.net\n",
    "#Make sure it's saved, then restart Ubuntu\n",
    "# if need to close many open Chrome windwos, run: killall chrome "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chrome_options = Options()\n",
    "\n",
    "prefs = {'profile.managed_default_content_settings.images': 2}\n",
    "chrome_options.add_experimental_option('prefs', prefs)\n",
    "\n",
    "# uBlock Origin\n",
    "chrome_options.add_extension('/Users/elipickh/Downloads/extension_1_13_2.crx')\n",
    "# Forbes Splash Screen Bypass\n",
    "chrome_options.add_extension('/Users/elipickh/Downloads/extension_0_1.crx')\n",
    "chrome_options.add_argument('--dns-prefetch-disable')\n",
    "#chrome_options.add_argument('headless')\n",
    "chrome_options.add_argument('window-position=0,0')\n",
    "# Make sure this restricted size below doesn't intefere/change the way items load!\n",
    "chrome_options.add_argument('window-size=600,800')\n",
    "\n",
    "chromedriver = '/Users/elipickh/Downloads/chromedriver'\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "\n",
    "#driver.implicitly_wait(0.5)\n",
    "\n",
    "def driver_article(url):\n",
    "    for x in range(0, 2):\n",
    "        try:\n",
    "            driver = webdriver.Chrome(chromedriver, chrome_options=chrome_options)\n",
    "            driver.get(url)\n",
    "            views = driver.find_element_by_css_selector('.view-count').text\n",
    "            main_cat = driver.find_element_by_css_selector('.crumb').text\n",
    "            try:\n",
    "                sub_cat = driver.find_element_by_css_selector('.hashtag').text\n",
    "            except:\n",
    "                sub_cat = ''\n",
    "            forbes_output_selenium = {'url':url, 'views':views, 'main_cat':main_cat, 'sub_cat':sub_cat}\n",
    "            driver.close()\n",
    "            return forbes_output_selenium\n",
    "        except:\n",
    "            pass\n",
    "        finally:\n",
    "            try:\n",
    "                driver.close()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "forbes_urls.sort()\n",
    "FNULL = open(os.devnull, 'w')\n",
    "os.chdir('home/ubuntu/Desktop/output/')\n",
    "for i in trange(0, 200):\n",
    "    # n_jobs=36 is fastest, but might crash. 12 is safer\n",
    "    results = Parallel(n_jobs=36)(delayed(driver_article)(url) for url in tqdm(chunkify(forbes_urls, 200)[i]))\n",
    "    time.sleep(1)\n",
    "    results = [x for x in results if x is not None]\n",
    "    results_df = pd.DataFrame(results)\n",
    "    time.sleep(1)\n",
    "    results_df.to_csv('forbes_selenium_output' + str(i) + '.csv', index = False)\n",
    "    a = subprocess.call('killall -9 chrome', stdout = FNULL, stedrr = subprocess.STDOUT, shell = True)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "articles_final = pd.read_csv('forbes_output_list_1.csv')\n",
    "articles_final = articles_final.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "views_final = pd.read_csv(\"views_final.csv\", thousands = ',')\n",
    "views_final= views_final.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forbes_combined_output = articles_final.merge(views_final, on='url', how = 'left')\n",
    "forbes_combined_output = forbes_combined_output.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(forbes_combined_output))\n",
    "print(len(articles_final))\n",
    "print(len(views_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forbes_combined_output.to_csv('forbes_combined_output.csv.gz', index = False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN URL's and article content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chrome_options = Options()\n",
    "#chrome_options.binary_location = '/Applications/Google Chrome Canary.app/Contents/MacOS/Google Chrome Canary'\n",
    "prefs = {'profile.managed_default_content_settings.images': 2}\n",
    "chrome_options.add_experimental_option('prefs', prefs)\n",
    "chrome_options.add_argument('--dns-prefetch-disable')\n",
    "chromedriver = '/Users/elipickh/Downloads/chromedriver'\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "\n",
    "driver = webdriver.Chrome(chromedriver) \n",
    "\n",
    "appended_data = []\n",
    "appended_data_0 = []\n",
    "appended_data_1 = []\n",
    "\n",
    "\n",
    "first = True\n",
    "bot_warn = 'Our systems have detected unusual traffic from your computer network'\n",
    "\n",
    "#for x in range(0, 2):\n",
    "for x in range(0, 1):    \n",
    "    for y in range(4, 8):\n",
    "        for m in range (1, 13):\n",
    "            for d in range(1, 32):\n",
    "                wait_for_internet_connection()\n",
    "                driver.get('https://www.google.com/search?q=' + str(uuid.uuid4()))\n",
    "                #time.sleep(random.uniform(1, 1.5))\n",
    "                wait_for_internet_connection()\n",
    "                driver.get('https://www.google.com/search?q=site:cnn.com/201' + str(y) +'/' + str('{0:02}'.format(m)) + '/' + str('{0:02}'.format(d)) + '/' +'&filter=' + str(x))\n",
    "                src = driver.page_source\n",
    "\n",
    "                if first:\n",
    "                    pause()\n",
    "                    first = False\n",
    "                elif bool(re.search(bot_warn, src)):\n",
    "                    os.system(\"printf '\\a'\"); time.sleep(1); os.system(\"printf '\\a'\"); time.sleep(1); os.system(\"printf '\\a'\")\n",
    "                    pause()\n",
    "                #else:\n",
    "                    #time.sleep(random.uniform(61, 63))\n",
    "                    #time.sleep(random.uniform(1, 1.5))\n",
    "\n",
    "                while True:\n",
    "                    for i in range(1, 150):\n",
    "                        try:\n",
    "                            \n",
    "                            url = driver.find_element_by_css_selector('.g:nth-child(' + str(i) +\") a\").get_attribute('href')\n",
    "                            appended_data.append(url)\n",
    "\n",
    "                        except:\n",
    "                            #print(\"no more links\")\n",
    "                            break\n",
    "                    #time.sleep(random.uniform(0.5, 1))\n",
    "\n",
    "                    try: \n",
    "                        # this is navigate to next page \n",
    "                        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                        time.sleep(random.uniform(0.5, 1))\n",
    "                        button = driver.find_element_by_css_selector('#pnnext .ch+ span')\n",
    "                        wait_for_internet_connection()\n",
    "                        button.click() \n",
    "                        #time.sleep(random.uniform(2, 3))\n",
    "                    except: \n",
    "                        #print(\"no more pages\")\n",
    "                        break\n",
    "                print(str(len(set(appended_data))) + '  ' + str(m) + '/' + str(d) + '--' + str(y))\n",
    "\n",
    "\n",
    "\n",
    "appended_data = set(appended_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn_urls = [s for s in appended_data if \"cnnespanol.cnn.com\" not in s]\n",
    "cnn_urls = [x.split('index.htm', 1)[0] for x in cnn_urls]\n",
    "cnn_urls = [x.split('?', 1)[0] for x in cnn_urls]\n",
    "cnn_urls = [x.split('comment-page-', 1)[0] for x in cnn_urls]\n",
    "cnn_urls = [x.replace('https:/','http:/') for x in cnn_urls]\n",
    "cnn_urls = [x.replace('www.','') for x in cnn_urls]\n",
    "cnn_urls = list(set(cnn_urls))\n",
    "print(len(cnn_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(cnn_urls, columns=[\"column\"])\n",
    "df.to_csv('cnn_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn_data = pd.read_csv(\"cnn_list.csv\", header=0)\n",
    "cnn_urls = list(cnn_data.column)\n",
    "cnn_urls = [x.split('%', 1)[0] for x in cnn_urls]\n",
    "len(set(cnn_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = Config()\n",
    "config.MAX_TEXT = 1000000\n",
    "\n",
    "def process_article(url):\n",
    "    for x in range(0, 5):\n",
    "        try:\n",
    "            article = Article(url, config)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "\n",
    "            headline = article.title\n",
    "            publish_date = article.publish_date\n",
    "            authors = article.authors\n",
    "            fulltext = article.text\n",
    "            article.nlp()\n",
    "            keywords = article.keywords\n",
    "            #summary = article.summary\n",
    "            cnn_all = {'url':url, \n",
    "                          'date':publish_date, \n",
    "                          'authors':authors, \n",
    "                          'headline':headline, \n",
    "                          'fulltext':fulltext, \n",
    "                          #'summary':summary, \n",
    "                          'keywords':keywords}\n",
    "            return cnn_all\n",
    "        except:\n",
    "            wait_for_internet_connection()\n",
    "            time.sleep(0.1)\n",
    "            continue\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "results = Parallel(n_jobs=-1)(delayed(process_article)(url) for url in tqdm(cnn_urls))\n",
    "\n",
    "results = [x for x in results if x is not None]\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('cnn_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn_output = pd.read_csv(\"cnn_output.csv\", header=0)\n",
    "cnn_output_list = cnn_output.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(s):\n",
    "    return str(str(s).encode('ascii','ignore'), 'utf-8').replace('\\n', ' ')\n",
    "\n",
    "for d in cnn_output_list:\n",
    "    for k, v in d.items():\n",
    "        if k=='fulltext' or k=='headline':\n",
    "            d[k]=clean_text(v)\n",
    "\n",
    "for d in cnn_output_list:\n",
    "    for k, v in list(d.items()):\n",
    "        if k=='fulltext':\n",
    "            d['sentences']=tokenize.sent_tokenize(v)\n",
    "\n",
    "for d in cnn_output_list:\n",
    "    for k, v in list(d.items()):\n",
    "        if k=='headline':\n",
    "            d['headline_score']=sid.polarity_scores(v)\n",
    "        if k=='sentences':\n",
    "            a=[]\n",
    "            for v in v:\n",
    "                a.append(sid.polarity_scores(v))  \n",
    "                d['fulltext_scores'] = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for maindict in cnn_output_list:\n",
    "    compound_list = []\n",
    "    pos_list =[]\n",
    "    neg_list =[]\n",
    "    neu_list =[]\n",
    "    compound_list_nonzero = []\n",
    "    pos_list_nonzero =[]\n",
    "    neg_list_nonzero =[]\n",
    "    neu_list_nonzero =[]    \n",
    "    for subdict in maindict['fulltext_scores']:\n",
    "        compound_list.append(subdict['compound'])\n",
    "        pos_list.append(subdict['pos'])\n",
    "        neg_list.append(subdict['neg'])\n",
    "        neu_list.append(subdict['neu'])\n",
    "        if subdict['compound']!=0:\n",
    "            compound_list_nonzero.append(subdict['compound'])\n",
    "        if subdict['pos']!=0:\n",
    "            pos_list_nonzero.append(subdict['pos'])\n",
    "        if subdict['neg']!=0:\n",
    "            neg_list_nonzero.append(subdict['neg'])\n",
    "        if subdict['neu']!=0:\n",
    "            neu_list_nonzero.append(subdict['neu'])  \n",
    "    \n",
    "    maindict['mean_compound'] = np.mean(compound_list)\n",
    "    maindict['median_compound'] = np.median(compound_list)\n",
    "    maindict['mean_pos'] = np.mean(pos_list)\n",
    "    maindict['median_pos'] = np.median(pos_list)\n",
    "    maindict['mean_neg'] = np.mean(neg_list)\n",
    "    maindict['median_neg'] = np.median(neg_list)\n",
    "    maindict['mean_neu'] = np.mean(neu_list)\n",
    "    maindict['median_neu'] = np.median(neu_list)\n",
    "    \n",
    "    maindict['mean_compound_nonzero'] = np.mean(compound_list_nonzero)\n",
    "    maindict['median_compound_nonzero'] = np.median(compound_list_nonzero)\n",
    "    maindict['mean_pos_nonzero'] = np.mean(pos_list_nonzero)\n",
    "    maindict['median_pos_nonzero'] = np.median(pos_list_nonzero)\n",
    "    maindict['mean_neg_nonzero'] = np.mean(neg_list_nonzero)\n",
    "    maindict['median_neg_nonzero'] = np.median(neg_list_nonzero)\n",
    "    maindict['mean_neu_nonzero'] = np.mean(neu_list_nonzero)\n",
    "    maindict['median_neu_nonzero'] = np.median(neu_list_nonzero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(cnn_output_list)\n",
    "df2.to_csv('cnn_output_list_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYTimes URL's and article content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chrome_options = Options()\n",
    "#chrome_options.binary_location = '/Applications/Google Chrome Canary.app/Contents/MacOS/Google Chrome Canary'\n",
    "prefs = {'profile.managed_default_content_settings.images': 2}\n",
    "chrome_options.add_experimental_option('prefs', prefs)\n",
    "chrome_options.add_argument('--dns-prefetch-disable')\n",
    "chromedriver = '/Users/elipickh/Downloads/chromedriver'\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "\n",
    "driver = webdriver.Chrome(chromedriver) \n",
    "\n",
    "appended_data = []\n",
    "appended_data_0 = []\n",
    "appended_data_1 = []\n",
    "\n",
    "first = True\n",
    "bot_warn = 'Our systems have detected unusual traffic from your computer network'\n",
    "\n",
    "#for x in range(0, 2):\n",
    "for x in range(0, 1):    \n",
    "    for y in range(4, 8):\n",
    "        for m in range (1, 13):\n",
    "            for d in range(1, 32):\n",
    "                wait_for_internet_connection()\n",
    "                driver.get('https://www.google.com/search?q=' + str(uuid.uuid4()))\n",
    "                #time.sleep(random.uniform(1, 1.5))\n",
    "                wait_for_internet_connection()\n",
    "                driver.get('https://www.google.com/search?q=site:nytimes.com/201' + str(y) +'/' + str('{0:02}'.format(m)) + '/' + str('{0:02}'.format(d)) + '/' +'&filter=' + str(x))\n",
    "                src = driver.page_source\n",
    "\n",
    "                if first:\n",
    "                    pause()\n",
    "                    first = False\n",
    "                elif bool(re.search(bot_warn, src)):\n",
    "                    os.system(\"printf '\\a'\"); time.sleep(1); os.system(\"printf '\\a'\"); time.sleep(1); os.system(\"printf '\\a'\")\n",
    "                    pause()\n",
    "                #else:\n",
    "                    #time.sleep(random.uniform(61, 63))\n",
    "                    #time.sleep(random.uniform(1, 1.5))\n",
    "\n",
    "                while True:\n",
    "                    for i in range(1, 150):\n",
    "                        try:\n",
    "                            \n",
    "                            url = driver.find_element_by_css_selector('.g:nth-child(' + str(i) +\") a\").get_attribute('href')\n",
    "                            appended_data.append(url)\n",
    "\n",
    "                        except:\n",
    "                            #print(\"no more links\")\n",
    "                            break\n",
    "                    #time.sleep(random.uniform(0.5, 1))\n",
    "\n",
    "                    try: \n",
    "                        # this is navigate to next page \n",
    "                        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                        time.sleep(random.uniform(0.5, 1))\n",
    "                        button = driver.find_element_by_css_selector('#pnnext .ch+ span')\n",
    "                        wait_for_internet_connection()\n",
    "                        button.click() \n",
    "                        #time.sleep(random.uniform(2, 3))\n",
    "                    except: \n",
    "                        #print(\"no more pages\")\n",
    "                        break\n",
    "                print(str(len(set(appended_data))) + '  ' + str(m) + '/' + str(d) + '--' + str(y))\n",
    "\n",
    "\n",
    "\n",
    "appended_data = set(appended_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nyt_urls = [x.replace('http:/','https:/') for x in appended_data]\n",
    "nyt_urls = [x.replace('mobile.nytimes.com','nytimes.com') for x in nyt_urls]\n",
    "nyt_urls = [x.replace('mwr.gtm.nytimes.com','nytimes.com') for x in nyt_urls]\n",
    "nyt_urls = [x.replace('mwr.nytimes.com','nytimes.com') for x in nyt_urls]\n",
    "nyt_urls = [x.replace('www.','') for x in nyt_urls]\n",
    "nyt_urls = list(set(nyt_urls))\n",
    "print(len(nyt_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(nyt_urls, columns=[\"column\"])\n",
    "df.to_csv('nyt_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(fetched articles in EC2... which gave nyt_output.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nyt_output = pd.read_csv(\"nyt_output.csv\", header=0)\n",
    "nyt_output_list = nyt_output.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def clean_text(s):\n",
    "    return str(str(s).encode('ascii','ignore'), 'utf-8').replace('\\n', ' ')\n",
    "\n",
    "for d in nyt_output_list:\n",
    "    for k, v in d.items():\n",
    "        if k=='fulltext' or k=='headline':\n",
    "            d[k]=clean_text(v)\n",
    "\n",
    "for d in nyt_output_list:\n",
    "    for k, v in list(d.items()):\n",
    "        if k=='fulltext':\n",
    "            d['sentences']=tokenize.sent_tokenize(v)\n",
    "\n",
    "for d in nyt_output_list:\n",
    "    for k, v in list(d.items()):\n",
    "        if k=='headline':\n",
    "            d['headline_score']=sid.polarity_scores(v)\n",
    "        if k=='sentences':\n",
    "            a=[]\n",
    "            for v in v:\n",
    "                a.append(sid.polarity_scores(v))  \n",
    "                d['fulltext_scores'] = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for maindict in nyt_output_list:\n",
    "    compound_list = []\n",
    "    pos_list =[]\n",
    "    neg_list =[]\n",
    "    neu_list =[]\n",
    "    compound_list_nonzero = []\n",
    "    pos_list_nonzero =[]\n",
    "    neg_list_nonzero =[]\n",
    "    neu_list_nonzero =[]    \n",
    "    for subdict in maindict['fulltext_scores']:\n",
    "        compound_list.append(subdict['compound'])\n",
    "        pos_list.append(subdict['pos'])\n",
    "        neg_list.append(subdict['neg'])\n",
    "        neu_list.append(subdict['neu'])\n",
    "        if subdict['compound']!=0:\n",
    "            compound_list_nonzero.append(subdict['compound'])\n",
    "        if subdict['pos']!=0:\n",
    "            pos_list_nonzero.append(subdict['pos'])\n",
    "        if subdict['neg']!=0:\n",
    "            neg_list_nonzero.append(subdict['neg'])\n",
    "        if subdict['neu']!=0:\n",
    "            neu_list_nonzero.append(subdict['neu'])  \n",
    "    \n",
    "    maindict['mean_compound'] = np.mean(compound_list)\n",
    "    maindict['median_compound'] = np.median(compound_list)\n",
    "    maindict['mean_pos'] = np.mean(pos_list)\n",
    "    maindict['median_pos'] = np.median(pos_list)\n",
    "    maindict['mean_neg'] = np.mean(neg_list)\n",
    "    maindict['median_neg'] = np.median(neg_list)\n",
    "    maindict['mean_neu'] = np.mean(neu_list)\n",
    "    maindict['median_neu'] = np.median(neu_list)\n",
    "    \n",
    "    maindict['mean_compound_nonzero'] = np.mean(compound_list_nonzero)\n",
    "    maindict['median_compound_nonzero'] = np.median(compound_list_nonzero)\n",
    "    maindict['mean_pos_nonzero'] = np.mean(pos_list_nonzero)\n",
    "    maindict['median_pos_nonzero'] = np.median(pos_list_nonzero)\n",
    "    maindict['mean_neg_nonzero'] = np.mean(neg_list_nonzero)\n",
    "    maindict['median_neg_nonzero'] = np.median(neg_list_nonzero)\n",
    "    maindict['mean_neu_nonzero'] = np.mean(neu_list_nonzero)\n",
    "    maindict['median_neu_nonzero'] = np.median(neu_list_nonzero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(nyt_output_list)\n",
    "df2.to_csv('nyt_output_list_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
